{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "invisible-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from time import sleep, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-strip",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "TensorFlow uses `Dataset` as dataloading interface. Parallelization and preprocessing is all handled via map-functions. For more details read the [documentation](https://www.tensorflow.org/guide/data#reading_input_data).\n",
    "\n",
    "In PyTorch we could choose between generator (iterator) and mapped dataset. In TensorFlow everything is assumed to be a generator dataset, which makes the map-concept natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rocky-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def _generator(start, step):\n",
    "        for i in range(start, 44, step):\n",
    "            # small data-prep. simulation delay\n",
    "            sleep(0.5)\n",
    "            d = tf.convert_to_tensor(np.array([i], dtype=np.float32).reshape(1,1,1), dtype=tf.float32)\n",
    "            yield d, -d\n",
    "\n",
    "    def __new__(cls, start=0, step=1):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_signature = (tf.TensorSpec(shape = (1,1,1), dtype = tf.float32), \n",
    "                                tf.TensorSpec(shape = (1,1,1), dtype = tf.float32)),\n",
    "            args = (start, step, )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-utilization",
   "metadata": {},
   "source": [
    "If we want to use the same amount of data i.e. 44 numbers and want to parallelize later on with 4 workers, we have to split the generators into 4 sub-generators, such that each generator only processes a 1/4 of the data. Otherwise we will just duplicate the data four times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stuffed-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MyDataset(start=0, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-circular",
   "metadata": {},
   "source": [
    "If we want to get batches, we just chain `.batch(batch_size)` to our `ds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "everyday-diary",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [0. 1. 2. 3. 4. 5. 6. 7.]\n",
      "y: [-0. -1. -2. -3. -4. -5. -6. -7.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [ 8.  9. 10. 11. 12. 13. 14. 15.]\n",
      "y: [ -8.  -9. -10. -11. -12. -13. -14. -15.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [16. 17. 18. 19. 20. 21. 22. 23.]\n",
      "y: [-16. -17. -18. -19. -20. -21. -22. -23.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [24. 25. 26. 27. 28. 29. 30. 31.]\n",
      "y: [-24. -25. -26. -27. -28. -29. -30. -31.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [32. 33. 34. 35. 36. 37. 38. 39.]\n",
      "y: [-32. -33. -34. -35. -36. -37. -38. -39.]\n",
      "-----------------------------------------------\n",
      "(4, 1, 1, 1) (4, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [40. 41. 42. 43.]\n",
      "y: [-40. -41. -42. -43.]\n",
      "-----------------------------------------------\n",
      "23.438462495803833\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "for x, y in ds.batch(8):\n",
    "    # Processing time of one batch\n",
    "    sleep(0.2)\n",
    "    print(x.shape, y.shape)\n",
    "    print('-----------------------------------------------')\n",
    "    print('x: {}'.format(x[:,0,0,0]))\n",
    "    print('y: {}'.format(y[:,0,0,0]))\n",
    "    print('-----------------------------------------------')\n",
    "\n",
    "end_time= time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-survey",
   "metadata": {},
   "source": [
    "## Parallel Loading\n",
    "\n",
    "By default everything is sequential i.e. while we wait for the batch to be processed (0.2 seconds) nothing else is happening. We can set use `prefetch` to load data while the current batch is processed.\n",
    "\n",
    "`tf.data.AUTOTUNE` lets tf decide how large the `prefetch` buffer should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "recovered-brain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [0. 1. 2. 3. 4. 5. 6. 7.]\n",
      "y: [-0. -1. -2. -3. -4. -5. -6. -7.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [ 8.  9. 10. 11. 12. 13. 14. 15.]\n",
      "y: [ -8.  -9. -10. -11. -12. -13. -14. -15.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [16. 17. 18. 19. 20. 21. 22. 23.]\n",
      "y: [-16. -17. -18. -19. -20. -21. -22. -23.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [24. 25. 26. 27. 28. 29. 30. 31.]\n",
      "y: [-24. -25. -26. -27. -28. -29. -30. -31.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [32. 33. 34. 35. 36. 37. 38. 39.]\n",
      "y: [-32. -33. -34. -35. -36. -37. -38. -39.]\n",
      "-----------------------------------------------\n",
      "(4, 1, 1, 1) (4, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [40. 41. 42. 43.]\n",
      "y: [-40. -41. -42. -43.]\n",
      "-----------------------------------------------\n",
      "22.33574676513672\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "for x, y in ds.batch(8).prefetch(buffer_size=tf.data.AUTOTUNE):\n",
    "    # Processing time of one batch\n",
    "    sleep(0.2)\n",
    "    print(x.shape, y.shape)\n",
    "    print('-----------------------------------------------')\n",
    "    print('x: {}'.format(x[:,0,0,0]))\n",
    "    print('y: {}'.format(y[:,0,0,0]))\n",
    "    print('-----------------------------------------------')\n",
    "\n",
    "end_time= time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-wrestling",
   "metadata": {},
   "source": [
    "This helped a little bit, but the real speed-up comes from parallel dataloading. \n",
    "For this we use `interleave` and create four datasets starting at 0, 1, 2 and 3 with a step-size of 4 and set `num_parallel_calls=4`.\n",
    "\n",
    "This creates four datasets:\n",
    "* d_0 = [0, 4, 8, ...]\n",
    "* d_1 = [1, 5, 9, ...]\n",
    "* d_2 = [2, 6, 10, ...]\n",
    "* d_3 = [3, 7, 11, ...]\n",
    "\n",
    "And interleaves them into [0, 1, 2, 3, 4, 5, ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dramatic-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = Dataset.range(4).interleave(lambda x: MyDataset(start=x, step=4), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fresh-techno",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [0. 1. 2. 3. 4. 5. 6. 7.]\n",
      "y: [-0. -1. -2. -3. -4. -5. -6. -7.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [ 8.  9. 10. 11. 12. 13. 14. 15.]\n",
      "y: [ -8.  -9. -10. -11. -12. -13. -14. -15.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [16. 17. 18. 19. 20. 21. 22. 23.]\n",
      "y: [-16. -17. -18. -19. -20. -21. -22. -23.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [24. 25. 26. 27. 28. 29. 30. 31.]\n",
      "y: [-24. -25. -26. -27. -28. -29. -30. -31.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [32. 33. 34. 35. 36. 37. 38. 39.]\n",
      "y: [-32. -33. -34. -35. -36. -37. -38. -39.]\n",
      "-----------------------------------------------\n",
      "(4, 1, 1, 1) (4, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [40. 41. 42. 43.]\n",
      "y: [-40. -41. -42. -43.]\n",
      "-----------------------------------------------\n",
      "5.770774841308594\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "for x, y in ids.batch(8).prefetch(tf.data.AUTOTUNE):\n",
    "    sleep(0.2)\n",
    "    print(x.shape, y.shape)\n",
    "    print('-----------------------------------------------')\n",
    "    print('x: {}'.format(x[:,0,0,0]))\n",
    "    print('y: {}'.format(y[:,0,0,0]))\n",
    "    print('-----------------------------------------------')\n",
    "\n",
    "end_time= time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-technology",
   "metadata": {},
   "source": [
    "## Shuffle\n",
    "\n",
    "True shuffling is not supported since the dataset is an iterator. But TensorFlow can buffer a given number of samples and then shuffle them. N.B. one could set the buffer size equal to the dataset size, but this requires more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "educational-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling each sub-dataset\n",
    "ids = Dataset.range(4).interleave(lambda x: MyDataset(start=x, step=4).shuffle(8), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "composite-darwin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [ 0. 25. 10. 31. 16.  5. 30. 19.]\n",
      "y: [ -0. -25. -10. -31. -16.  -5. -30. -19.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [12. 37. 34. 11.  8. 13. 38.  7.]\n",
      "y: [-12. -37. -34. -11.  -8. -13. -38.  -7.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [40. 33. 26.  3. 28.  9. 42. 39.]\n",
      "y: [-40. -33. -26.  -3. -28.  -9. -42. -39.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [20. 29. 18. 43. 32. 21. 14. 15.]\n",
      "y: [-20. -29. -18. -43. -32. -21. -14. -15.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [ 4.  1.  6. 27. 36. 17.  2. 35.]\n",
      "y: [ -4.  -1.  -6. -27. -36. -17.  -2. -35.]\n",
      "-----------------------------------------------\n",
      "(4, 1, 1, 1) (4, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [24. 41. 22. 23.]\n",
      "y: [-24. -41. -22. -23.]\n",
      "-----------------------------------------------\n",
      "6.6167871952056885\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "for x, y in ids.batch(8).prefetch(tf.data.AUTOTUNE):\n",
    "    sleep(0.2)\n",
    "    print(x.shape, y.shape)\n",
    "    print('-----------------------------------------------')\n",
    "    print('x: {}'.format(x[:,0,0,0]))\n",
    "    print('y: {}'.format(y[:,0,0,0]))\n",
    "    print('-----------------------------------------------')\n",
    "\n",
    "end_time= time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-classics",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "\n",
    "If we want to use data-augmentation (or do anything more with the data) we can just map another function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fantastic-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_gaussian_noise(x, y):\n",
    "    [x_noisy] = tf.py_function(np.random.normal, [x], [tf.float32])\n",
    "    x_noisy.set_shape(x.shape)\n",
    "    return x_noisy, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "first-volunteer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [ 7.45624   12.732648   2.7712266 22.648294  20.965918   5.335439\n",
      " 14.361323  30.90427  ]\n",
      "y: [ -8. -13.  -2. -23. -20.  -5. -14. -31.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [22.944181   7.991416  11.431693  20.480597  27.774282  27.849373\n",
      "  6.2662497  2.111086 ]\n",
      "y: [-24.  -9. -10. -19. -28. -29.  -6.  -3.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [14.42896    1.5938652 30.513695  15.94892    3.620058  37.46703\n",
      " 22.975163  35.45491  ]\n",
      "y: [-16.  -1. -30. -15.  -4. -37. -22. -35.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [35.08035  21.495703 33.10992  38.765636 11.589279 41.005302 23.525087\n",
      " 25.637701]\n",
      "y: [-36. -21. -34. -39. -12. -41. -26. -27.]\n",
      "-----------------------------------------------\n",
      "(8, 1, 1, 1) (8, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [ 0.2473696 26.097044  18.057276  40.938404  29.377256  32.67736\n",
      " 38.791428  11.56435  ]\n",
      "y: [ -0. -25. -18. -43. -32. -33. -38. -11.]\n",
      "-----------------------------------------------\n",
      "(4, 1, 1, 1) (4, 1, 1, 1)\n",
      "-----------------------------------------------\n",
      "x: [39.130592  17.227192  42.9525     7.5448422]\n",
      "y: [-40. -17. -42.  -7.]\n",
      "-----------------------------------------------\n",
      "6.651840686798096\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "for x, y in ids.map(tf_gaussian_noise).batch(8).prefetch(tf.data.AUTOTUNE):\n",
    "    sleep(0.2)\n",
    "    print(x.shape, y.shape)\n",
    "    print('-----------------------------------------------')\n",
    "    print('x: {}'.format(x[:,0,0,0]))\n",
    "    print('y: {}'.format(y[:,0,0,0]))\n",
    "    print('-----------------------------------------------')\n",
    "\n",
    "end_time= time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-nightmare",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
